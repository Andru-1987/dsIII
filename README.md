# CLASE 2

## NLP I

# üìå **¬øQu√© es el NLP (Procesamiento de Lenguaje Natural)?**  
El **Procesamiento de Lenguaje Natural (NLP - Natural Language Processing)** es una rama de la inteligencia artificial que permite a las computadoras entender, interpretar y generar lenguaje humano de manera autom√°tica.  

El NLP combina ling√º√≠stica y aprendizaje autom√°tico para trabajar con **texto** y **voz**, ayudando a las m√°quinas a interactuar con los humanos de manera m√°s eficiente.  

---

## üîπ **Caracter√≠sticas clave del NLP**  

### 1Ô∏è‚É£ **Tokenizaci√≥n**  
- Divide un texto en palabras o frases m√°s peque√±as llamadas **tokens**.  
- Ejemplo:  
  ```
  Texto: "El gato duerme"
  Tokens: ["El", "gato", "duerme"]
  ```

### 2Ô∏è‚É£ **Lematizaci√≥n y stemming**  
- **Stemming:** Reduce una palabra a su ra√≠z base eliminando sufijos.  
  - Ejemplo: "corriendo" ‚Üí "corr"  
- **Lematizaci√≥n:** Convierte una palabra a su forma base considerando su significado.  
  - Ejemplo: "corriendo" ‚Üí "correr"  

### 3Ô∏è‚É£ **An√°lisis morfosint√°ctico (POS Tagging)**  
- Identifica el tipo de cada palabra (**sustantivo, verbo, adjetivo, etc.**).  
- Ejemplo:  
  ```
  "El gato duerme"
  "El" (determinante), "gato" (sustantivo), "duerme" (verbo)
  ```

### 4Ô∏è‚É£ **Reconocimiento de entidades (NER - Named Entity Recognition)**  
- Identifica nombres de personas, lugares, fechas, organizaciones, etc.  
- Ejemplo:  
  ```
  "Apple lanz√≥ el iPhone 15 en septiembre de 2023"
  - Apple ‚Üí Empresa
  - iPhone 15 ‚Üí Producto
  - septiembre de 2023 ‚Üí Fecha
  ```

### 5Ô∏è‚É£ **An√°lisis de sentimiento**  
- Determina si un texto tiene un tono **positivo, negativo o neutral**.  
- Se usa en redes sociales, servicio al cliente, an√°lisis de opiniones, etc.  

### 6Ô∏è‚É£ **Bag of Words y TF-IDF**  
- M√©todos para representar texto en formato num√©rico.  
- **Bag of Words (BoW):** Cuenta la frecuencia de palabras en un documento.  
- **TF-IDF:** Asigna pesos a palabras seg√∫n su importancia en un conjunto de documentos.  

### 7Ô∏è‚É£ **N-grams**  
- Secuencias de **N palabras consecutivas** usadas para modelar el lenguaje.  
- Ejemplo:  
  ```
  "El gato duerme en la casa"
  - Bigramas: ["El gato", "gato duerme", "duerme en"]
  ```

### 8Ô∏è‚É£ **Modelos avanzados de NLP**  
- **Word Embeddings (Word2Vec, GloVe, FastText):** Representan palabras en vectores num√©ricos capturando su significado.  
- **Transformers (BERT, GPT, T5):** Modelos m√°s sofisticados que entienden contexto y generan texto de manera fluida.  

---

## üìå **Resumen del NLP**  

| Caracter√≠stica | Descripci√≥n |
|--------------|------------|
| **Tokenizaci√≥n** | Divide texto en palabras o frases. |
| **Lematizaci√≥n y Stemming** | Normaliza palabras a su forma base. |
| **An√°lisis gramatical (POS Tagging)** | Identifica sustantivos, verbos, adjetivos, etc. |
| **Reconocimiento de entidades (NER)** | Detecta nombres de personas, lugares, fechas, etc. |
| **An√°lisis de sentimiento** | Determina si un texto es positivo, negativo o neutral. |
| **Bag of Words y TF-IDF** | Representan texto en formato num√©rico. |
| **N-grams** | Modela texto usando secuencias de palabras consecutivas. |
| **Word Embeddings y Transformers** | M√©todos avanzados para capturar significado del lenguaje. |

---


## CHAT GPT

El algoritmo detr√°s de ChatGPT es un **modelo de lenguaje basado en redes neuronales profundas**, espec√≠ficamente una variante de **GPT (Generative Pre-trained Transformer)** desarrollado por OpenAI.  

### üîπ **Principales caracter√≠sticas del algoritmo de ChatGPT**  

1. **Basado en la arquitectura Transformer**  
   - Utiliza el mecanismo de **atenci√≥n** (Self-Attention) para analizar el contexto de las palabras en una conversaci√≥n y generar respuestas coherentes.  

2. **Entrenamiento en dos fases:**  
   - **Pre-entrenamiento:** Se entrena con grandes vol√∫menes de texto de internet para aprender patrones del lenguaje.  
   - **Ajuste fino (Fine-Tuning):** Se refina con datos espec√≠ficos y, en algunos casos, con aprendizaje por refuerzo con retroalimentaci√≥n humana (**RLHF - Reinforcement Learning from Human Feedback**).  

3. **Generaci√≥n de texto autoregresiva**  
   - Predice la siguiente palabra en funci√≥n del contexto previo.  

4. **Uso de embeddings de palabras**  
   - Representa palabras y frases en un espacio matem√°tico para entender relaciones sem√°nticas.  

5. **Tama√±o del modelo**  
   - Existen versiones con diferentes cantidades de par√°metros (GPT-3, GPT-3.5, GPT-4, etc.), donde los modelos m√°s grandes tienen m√°s capacidad de comprensi√≥n y generaci√≥n de texto.  

6. **Optimizaci√≥n con RLHF**  
   - Se mejora la calidad de las respuestas a trav√©s de un proceso donde entrenadores humanos califican y ajustan las respuestas del modelo.  

### üîπ **Diferencia con otros modelos**  
- **Vs RNN/LSTM:** ChatGPT usa Transformers, que manejan dependencias a largo plazo mejor que RNNs o LSTMs.  
- **Vs Modelos Basados en Reglas:** No sigue reglas preprogramadas, sino que aprende patrones del lenguaje.  

---

### REGEXP

# üìå **Mini Tutorial de Expresiones Regulares (RegEx) para NLP**  

Las **expresiones regulares (RegEx)** son herramientas poderosas para procesar y manipular texto en **NLP (Procesamiento de Lenguaje Natural)**. Se usan para extraer informaci√≥n, limpiar datos y estructurar texto de manera eficiente.  

---

## üìå **1Ô∏è‚É£ Conceptos B√°sicos de RegEx**  

| **Patr√≥n** | **Significado** | **Ejemplo Coincidente** |
|-----------|----------------|-------------------------|
| `.`       | Cualquier car√°cter (excepto nueva l√≠nea) | `"casa"`, `"mesa"` ‚Üí `".asa"` |
| `\d`      | Un d√≠gito (0-9) | `"123"` ‚Üí `"\d\d\d"` |
| `\w`      | Cualquier car√°cter alfanum√©rico (`a-z, A-Z, 0-9, _`) | `"Python_3"` ‚Üí `"\w+"` |
| `\s`      | Espacio en blanco (espacio, tabulaci√≥n, salto de l√≠nea) | `"Hola mundo"` ‚Üí `"Hola\s"` |
| `\b`      | L√≠mite de palabra | `"el gato"` ‚Üí `"\bel\b"` (solo coincide "el") |
| `+`       | Uno o m√°s repeticiones | `"aaa"`, `"abc"` ‚Üí `"a+"` |
| `*`       | Cero o m√°s repeticiones | `""`, `"aaa"` ‚Üí `"a*"` |
| `?`       | Cero o una repetici√≥n | `"color"`, `"colour"` ‚Üí `"colou?r"` |
| `{n,m}`   | Entre `n` y `m` repeticiones | `"12345"` ‚Üí `"\d{2,4}"` (coincide con "1234") |
| `[]`      | Grupo de caracteres | `"casa"`, `"cosa"` ‚Üí `"[cC]asa"` |
| `\|`       | Operador "o" | `"gato"`, `"perro"` ‚Üí `"gato\|perro"` |
| `()`      | Grupo de captura | `"abc123"` ‚Üí `"(abc)\d+"` (captura "abc") |

---

## üìå **2Ô∏è‚É£ Aplicaciones en NLP**  

### üìù **1. Eliminaci√≥n de Puntuaci√≥n**  
```python
import re

texto = "¬°Hola! ¬øC√≥mo est√°s? Me llamo Andru."
texto_limpio = re.sub(r"[^\w\s]", "", texto)  # Remueve puntuaci√≥n
print(texto_limpio)
```
üîπ **Salida:**  
```
Hola C√≥mo est√°s Me llamo Andru
```

---

### üî¢ **2. Extraer N√∫meros de un Texto**  
```python
texto = "Tengo 3 gatos y 2 perros."
numeros = re.findall(r"\d+", texto)
print(numeros)
```
üîπ **Salida:**  
```
['3', '2']
```

---

### üìß **3. Validar Correos Electr√≥nicos**  
```python
def es_email_valido(email):
    patron = r"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$"
    return bool(re.match(patron, email))

print(es_email_valido("correo@dominio.com"))  # True
print(es_email_valido("correo@dominio"))      # False
```

---

### üìÖ **4. Extraer Fechas**  
```python
texto = "Las reuniones son el 15/08/2023 y el 20-09-2024."
fechas = re.findall(r"\b\d{2}[/\-]\d{2}[/\-]\d{4}\b", texto)
print(fechas)
```
üîπ **Salida:**  
```
['15/08/2023', '20-09-2024']
```

---

### üè∑ **5. Tokenizaci√≥n de Texto con RegEx**  
```python
texto = "El NLP es incre√≠ble, ¬øno crees?"
tokens = re.findall(r"\b\w+\b", texto)
print(tokens)
```
üîπ **Salida:**  
```
['El', 'NLP', 'es', 'incre√≠ble', 'no', 'crees']
```

---

[Curso de expresiones regulares](https://www.youtube.com/watch?v=de_7k4SHEO0&t=2s)


---


 