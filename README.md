# CLASE 2

## NLP I

# üìå **¬øQu√© es el NLP (Procesamiento de Lenguaje Natural)?**  
El **Procesamiento de Lenguaje Natural (NLP - Natural Language Processing)** es una rama de la inteligencia artificial que permite a las computadoras entender, interpretar y generar lenguaje humano de manera autom√°tica.  

El NLP combina ling√º√≠stica y aprendizaje autom√°tico para trabajar con **texto** y **voz**, ayudando a las m√°quinas a interactuar con los humanos de manera m√°s eficiente.  

---

## üîπ **Caracter√≠sticas clave del NLP**  

### 1Ô∏è‚É£ **Tokenizaci√≥n**  
- Divide un texto en palabras o frases m√°s peque√±as llamadas **tokens**.  
- Ejemplo:  
  ```
  Texto: "El gato duerme"
  Tokens: ["El", "gato", "duerme"]
  ```

### 2Ô∏è‚É£ **Lematizaci√≥n y stemming**  
- **Stemming:** Reduce una palabra a su ra√≠z base eliminando sufijos.  
  - Ejemplo: "corriendo" ‚Üí "corr"  
- **Lematizaci√≥n:** Convierte una palabra a su forma base considerando su significado.  
  - Ejemplo: "corriendo" ‚Üí "correr"  

### 3Ô∏è‚É£ **An√°lisis morfosint√°ctico (POS Tagging)**  
- Identifica el tipo de cada palabra (**sustantivo, verbo, adjetivo, etc.**).  
- Ejemplo:  
  ```
  "El gato duerme"
  "El" (determinante), "gato" (sustantivo), "duerme" (verbo)
  ```

### 4Ô∏è‚É£ **Reconocimiento de entidades (NER - Named Entity Recognition)**  
- Identifica nombres de personas, lugares, fechas, organizaciones, etc.  
- Ejemplo:  
  ```
  "Apple lanz√≥ el iPhone 15 en septiembre de 2023"
  - Apple ‚Üí Empresa
  - iPhone 15 ‚Üí Producto
  - septiembre de 2023 ‚Üí Fecha
  ```

### 5Ô∏è‚É£ **An√°lisis de sentimiento**  
- Determina si un texto tiene un tono **positivo, negativo o neutral**.  
- Se usa en redes sociales, servicio al cliente, an√°lisis de opiniones, etc.  

### 6Ô∏è‚É£ **Bag of Words y TF-IDF**  
- M√©todos para representar texto en formato num√©rico.  
- **Bag of Words (BoW):** Cuenta la frecuencia de palabras en un documento.  
- **TF-IDF:** Asigna pesos a palabras seg√∫n su importancia en un conjunto de documentos.  

### 7Ô∏è‚É£ **N-grams**  
- Secuencias de **N palabras consecutivas** usadas para modelar el lenguaje.  
- Ejemplo:  
  ```
  "El gato duerme en la casa"
  - Bigramas: ["El gato", "gato duerme", "duerme en"]
  ```

### 8Ô∏è‚É£ **Modelos avanzados de NLP**  
- **Word Embeddings (Word2Vec, GloVe, FastText):** Representan palabras en vectores num√©ricos capturando su significado.  
- **Transformers (BERT, GPT, T5):** Modelos m√°s sofisticados que entienden contexto y generan texto de manera fluida.  

---

## üìå **Resumen del NLP**  

| Caracter√≠stica | Descripci√≥n |
|--------------|------------|
| **Tokenizaci√≥n** | Divide texto en palabras o frases. |
| **Lematizaci√≥n y Stemming** | Normaliza palabras a su forma base. |
| **An√°lisis gramatical (POS Tagging)** | Identifica sustantivos, verbos, adjetivos, etc. |
| **Reconocimiento de entidades (NER)** | Detecta nombres de personas, lugares, fechas, etc. |
| **An√°lisis de sentimiento** | Determina si un texto es positivo, negativo o neutral. |
| **Bag of Words y TF-IDF** | Representan texto en formato num√©rico. |
| **N-grams** | Modela texto usando secuencias de palabras consecutivas. |
| **Word Embeddings y Transformers** | M√©todos avanzados para capturar significado del lenguaje. |

---


## CHAT GPT

El algoritmo detr√°s de ChatGPT es un **modelo de lenguaje basado en redes neuronales profundas**, espec√≠ficamente una variante de **GPT (Generative Pre-trained Transformer)** desarrollado por OpenAI.  

[DNN](https://botpress.com/es/blog/deep-neural-network)

### üîπ **Principales caracter√≠sticas del algoritmo de ChatGPT**  

1. **Basado en la arquitectura Transformer**  
   - Utiliza el mecanismo de **atenci√≥n** (Self-Attention) para analizar el contexto de las palabras en una conversaci√≥n y generar respuestas coherentes.  

2. **Entrenamiento en dos fases:**  
   - **Pre-entrenamiento:** Se entrena con grandes vol√∫menes de texto de internet para aprender patrones del lenguaje.  
   - **Ajuste fino (Fine-Tuning):** Se refina con datos espec√≠ficos y, en algunos casos, con aprendizaje por refuerzo con retroalimentaci√≥n humana (**RLHF - Reinforcement Learning from Human Feedback**).  

3. **Generaci√≥n de texto autoregresiva**  
   - Predice la siguiente palabra en funci√≥n del contexto previo.  

4. **Uso de embeddings de palabras**  
   - Representa palabras y frases en un espacio matem√°tico para entender relaciones sem√°nticas.  

5. **Tama√±o del modelo**  
   - Existen versiones con diferentes cantidades de par√°metros (GPT-3, GPT-3.5, GPT-4, etc.), donde los modelos m√°s grandes tienen m√°s capacidad de comprensi√≥n y generaci√≥n de texto.  

6. **Optimizaci√≥n con RLHF**  
   - Se mejora la calidad de las respuestas a trav√©s de un proceso donde entrenadores humanos califican y ajustan las respuestas del modelo.  

### üîπ **Diferencia con otros modelos**  
- **Vs RNN/LSTM:** ChatGPT usa Transformers, que manejan dependencias a largo plazo mejor que RNNs o LSTMs.  
- **Vs Modelos Basados en Reglas:** No sigue reglas preprogramadas, sino que aprende patrones del lenguaje.  

---

### REGEXP

# üìå **Mini Tutorial de Expresiones Regulares (RegEx) para NLP**  

Las **expresiones regulares (RegEx)** son herramientas poderosas para procesar y manipular texto en **NLP (Procesamiento de Lenguaje Natural)**. Se usan para extraer informaci√≥n, limpiar datos y estructurar texto de manera eficiente.  

---

## üìå **1Ô∏è‚É£ Conceptos B√°sicos de RegEx**  

| **Patr√≥n** | **Significado** | **Ejemplo Coincidente** |
|-----------|----------------|-------------------------|
| `.`       | Cualquier car√°cter (excepto nueva l√≠nea) | `"casa"`, `"mesa"` ‚Üí `".asa"` |
| `\d`      | Un d√≠gito (0-9) | `"123"` ‚Üí `"\d\d\d"` |
| `\w`      | Cualquier car√°cter alfanum√©rico (`a-z, A-Z, 0-9, _`) | `"Python_3"` ‚Üí `"\w+"` |
| `\s`      | Espacio en blanco (espacio, tabulaci√≥n, salto de l√≠nea) | `"Hola mundo"` ‚Üí `"Hola\s"` |
| `\b`      | L√≠mite de palabra | `"el gato"` ‚Üí `"\bel\b"` (solo coincide "el") |
| `+`       | Uno o m√°s repeticiones | `"aaa"`, `"abc"` ‚Üí `"a+"` |
| `*`       | Cero o m√°s repeticiones | `""`, `"aaa"` ‚Üí `"a*"` |
| `?`       | Cero o una repetici√≥n | `"color"`, `"colour"` ‚Üí `"colou?r"` |
| `{n,m}`   | Entre `n` y `m` repeticiones | `"12345"` ‚Üí `"\d{2,4}"` (coincide con "1234") |
| `[]`      | Grupo de caracteres | `"casa"`, `"cosa"` ‚Üí `"[cC]asa"` |
| `\|`       | Operador "o" | `"gato"`, `"perro"` ‚Üí `"gato\|perro"` |
| `()`      | Grupo de captura | `"abc123"` ‚Üí `"(abc)\d+"` (captura "abc") |

---

## üìå **2Ô∏è‚É£ Aplicaciones en NLP**  

### üìù **1. Eliminaci√≥n de Puntuaci√≥n**  
```python
import re

texto = "¬°Hola! ¬øC√≥mo est√°s? Me llamo Andru."
texto_limpio = re.sub(r"[^\w\s]", "", texto)  # Remueve puntuaci√≥n
print(texto_limpio)
```
üîπ **Salida:**  
```
Hola C√≥mo est√°s Me llamo Andru
```

---

### üî¢ **2. Extraer N√∫meros de un Texto**  
```python
texto = "Tengo 3 gatos y 2 perros."
numeros = re.findall(r"\d+", texto)
print(numeros)
```
üîπ **Salida:**  
```
['3', '2']
```

---

### üìß **3. Validar Correos Electr√≥nicos**  
```python
def es_email_valido(email):
    patron = r"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$"
    return bool(re.match(patron, email))

print(es_email_valido("correo@dominio.com"))  # True
print(es_email_valido("correo@dominio"))      # False
```

---

### üìÖ **4. Extraer Fechas**  
```python
texto = "Las reuniones son el 15/08/2023 y el 20-09-2024."
fechas = re.findall(r"\b\d{2}[/\-]\d{2}[/\-]\d{4}\b", texto)
print(fechas)
```
üîπ **Salida:**  
```
['15/08/2023', '20-09-2024']
```

---

### üè∑ **5. Tokenizaci√≥n de Texto con RegEx**  
```python
texto = "El NLP es incre√≠ble, ¬øno crees?"
tokens = re.findall(r"\b\w+\b", texto)
print(tokens)
```
üîπ **Salida:**  
```
['El', 'NLP', 'es', 'incre√≠ble', 'no', 'crees']
```

---

[Curso de expresiones regulares](https://www.youtube.com/watch?v=de_7k4SHEO0&t=2s)


---
Usos m√°s comunes de la t√©cnica **Bag of Words (BoW)** en procesamiento de lenguaje natural (NLP):

| Uso Com√∫n                      | Descripci√≥n                                                                                     |
|--------------------------------|-------------------------------------------------------------------------------------------------|
| **Clasificaci√≥n de Texto**     | Se utiliza para categorizar documentos o rese√±as en diferentes clases (ej. positivo/negativo).|
| **An√°lisis de Sentimientos**   | Permite identificar el sentimiento detr√°s de un texto, clasific√°ndolo en categor√≠as emocionales.|
| **Extracci√≥n de Caracter√≠sticas** | Se usa para convertir texto en vectores num√©ricos, facilitando su uso en algoritmos de machine learning.|
| **Modelado de Temas**         | Ayuda a descubrir temas predominantes en un conjunto de documentos al analizar las palabras m√°s frecuentes. |
| **Recomendaciones de Contenido** | Puede mejorar sistemas de recomendaci√≥n al analizar la similitud entre textos (ej. art√≠culos, libros).|
| **B√∫squeda de Informaci√≥n**   | Facilita la indexaci√≥n y recuperaci√≥n de documentos relevantes en bases de datos o motores de b√∫squeda. |
| **An√°lisis de Tendencias**    | Permite identificar tendencias en el uso de palabras o frases a lo largo del tiempo en redes sociales o art√≠culos.|
| **Generaci√≥n de Res√∫menes**   | Ayuda a identificar las palabras clave y frases m√°s relevantes para crear res√∫menes autom√°ticos de textos largos. |
| **Detecci√≥n de Plagio**       | Se usa para comparar la similitud entre documentos y detectar contenido plagiado.              |
| **Filtrado de Spam**          | Ayuda a clasificar correos electr√≥nicos o mensajes como spam o no spam analizando las palabras clave. |


---

Claro, aqu√≠ tienes un ejemplo sencillo para ilustrar c√≥mo se calcula TF-IDF:

### Supongamos que tenemos un corpus de 3 documentos:

- **Documento 1 (D1)**: "El perro juega en el parque."
- **Documento 2 (D2)**: "El gato juega en la casa."
- **Documento 3 (D3)**: "El perro y el gato est√°n en casa."

### Paso 1: Calcular TF

Primero, contamos la frecuencia de cada t√©rmino en cada documento:

- **D1**:
  - "El": 1
  - "perro": 1
  - "juega": 1
  - "en": 1
  - "parque": 1
  - **Total**: 5

  \[
  TF(\text{"perro"}, D1) = \frac{1}{5} = 0.2
  \]
  \[
  TF(\text{"juega"}, D1) = \frac{1}{5} = 0.2
  \]

- **D2**:
  - "El": 1
  - "gato": 1
  - "juega": 1
  - "en": 1
  - "la": 1
  - "casa": 1
  - **Total**: 6

  \[
  TF(\text{"gato"}, D2) = \frac{1}{6} \approx 0.167
  \]
  \[
  TF(\text{"juega"}, D2) = \frac{1}{6} \approx 0.167
  \]

- **D3**:
  - "El": 1
  - "perro": 1
  - "y": 1
  - "gato": 1
  - "est√°n": 1
  - "en": 1
  - "casa": 1
  - **Total**: 7

  \[
  TF(\text{"perro"}, D3) = \frac{1}{7} \approx 0.143
  \]
  \[
  TF(\text{"gato"}, D3) = \frac{1}{7} \approx 0.143
  \]

### Paso 2: Calcular IDF

Ahora calculamos IDF para cada t√©rmino:

- **N√∫mero total de documentos (N)**: 3

- **Documentos que contienen cada t√©rmino**:
  - "perro": 2 (D1, D3)
  - "gato": 2 (D2, D3)
  - "juega": 2 (D1, D2)
  - "en": 3 (D1, D2, D3)
  - "parque": 1 (D1)
  - "casa": 2 (D2, D3)

Usamos la f√≥rmula para IDF:

\[
IDF(t) = \log\left(\frac{N}{|\{d \in D : t \in d\}|}\right)
\]

- **IDF("perro")**:
  \[
  IDF(\text{"perro"}) = \log\left(\frac{3}{2}\right) \approx 0.176
  \]

- **IDF("gato")**:
  \[
  IDF(\text{"gato"}) = \log\left(\frac{3}{2}\right) \approx 0.176
  \]

- **IDF("juega")**:
  \[
  IDF(\text{"juega"}) = \log\left(\frac{3}{2}\right) \approx 0.176
  \]

- **IDF("en")**:
  \[
  IDF(\text{"en"}) = \log\left(\frac{3}{3}\right) = 0
  \]

- **IDF("parque")**:
  \[
  IDF(\text{"parque"}) = \log\left(\frac{3}{1}\right) \approx 1.098
  \]

- **IDF("casa")**:
  \[
  IDF(\text{"casa"}) = \log\left(\frac{3}{2}\right) \approx 0.176
  \]

### Paso 3: Calcular TF-IDF

Finalmente, multiplicamos TF y IDF para cada t√©rmino en cada documento:

- **Para D1**:
  - **TF-IDF("perro", D1)**:
    \[
    TFIDF(\text{"perro"}, D1) = 0.2 \times 0.176 \approx 0.0352
    \]
  - **TF-IDF("juega", D1)**:
    \[
    TFIDF(\text{"juega"}, D1) = 0.2 \times 0.176 \approx 0.0352
    \]
  - **TF-IDF("parque", D1)**:
    \[
    TFIDF(\text{"parque"}, D1) = 0.2 \times 1.098 \approx 0.2196
    \]

- **Para D2**:
  - **TF-IDF("gato", D2)**:
    \[
    TFIDF(\text{"gato"}, D2) \approx 0.167 \times 0.176 \approx 0.0294
    \]
  - **TF-IDF("juega", D2)**:
    \[
    TFIDF(\text{"juega"}, D2) \approx 0.167 \times 0.176 \approx 0.0294
    \]

- **Para D3**:
  - **TF-IDF("perro", D3)**:
    \[
    TFIDF(\text{"perro"}, D3) \approx 0.143 \times 0.176 \approx 0.0252
    \]
  - **TF-IDF("gato", D3)**:
    \[
    TFIDF(\text{"gato"}, D3) \approx 0.143 \times 0.176 \approx 0.0252
    \]

Este es un ejemplo b√°sico de c√≥mo se calcula TF-IDF para un peque√±o corpus. En la pr√°ctica, trabajar√≠as con textos mucho m√°s grandes y usar√≠as herramientas y bibliotecas que faciliten estos c√°lculos.

